---
permalink: /
title: "Xinlei Yu (Leo)"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Xinlei (Leo) Yu is an incoming PhD student at University of Southern California, advised by Dr. Heather Culbertson. Currently, Xinlei works for Android XR as a software engineer. Xinlei's research interest includes haptics, ungrounded robots, and perceptions.

<br>
<br>





# On-going Project

<table>
<tr>
<td style="width:40%">
<img src="https://raw.githubusercontent.com/XinleiYu-Leo/XinleiYu-Leo.github.io/master/images/object_2.png" alt="teaser" height="200" width="800"/>
</td>
<td style="width:60%">
Working on end-to-end wearable navigation with autonomous ungrounded robots that guide users within indoor environments via haptic feedback:  <br>
<br>
</td>
</tr>
</table>
<br>
<br>

# Under Review

<table>
<tr>
<td style="width:40%">
<img src="https://raw.githubusercontent.com/XinleiYu-Leo/XinleiYu-Leo.github.io/master/images/crazy_teaser_new.png" alt="teaser" height="200" width="800"/>
</td>
<td style="width:60%">
<strong>CrazyJoystick: A Handheld Flyable Joystick for Providing On-Demand Haptic Feedback in Virtual Reality</strong> <br>
IEEE World Haptics Conference 2025
<br><br>
Yang Chen*, <strong>Xinlei Yu*</strong>, and Heather Culbertson
<br><br>
</td>
</tr>
</table>

<br>
<br>


<br>

# Fun Project 

<table>
<tr>
<td style="width:40%">
<img src="https://raw.githubusercontent.com/XinleiYu-Leo/XinleiYu-Leo.github.io/master/images/handtrackingDrone.drawio.png" alt="System Image" height="400" width="450" />
</td>
<td style="width:60%">
<strong>Gesture-based Controlled Crazyflie Hand Tracking System</strong>
<br>
<a href="https://youtube.com/shorts/QBKCI4z-H1E?feature=share">[Demo Video]</a> <a href="https://raw.githubusercontent.com/XinleiYu-Leo/XinleiYu-Leo.github.io/master/images/handtrackingDrone.drawio.png">[System Diagram]</a> 
</td>
</tr>
<tr>
<td style="width:40%">
<img src="https://raw.githubusercontent.com/XinleiYu-Leo/XinleiYu-Leo.github.io/master/images/VR_DressingRoom.png" alt="VR Room Image" height="300" width="450" />
</td>
<td style="width:60%">
<strong>VR Dressing Room</strong>
<br>
<a href="https://youtube.com/shorts/3uVC-7T6mHI?feature=share">[Demo Video]</a>
</td>
</tr>
</table>

<br>

# Past Research Project (selected)

<table>
  <tr>
    <td style="width:40%">
      <img src="https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/electro_diagram.png" alt="drawing2" height="250" width="500"/>
    </td>
    <td style="width:60%">
      <strong>TactileNet: Multi-armed bandit-based calibration for Electro-tactile Simulation:</strong> Developed an electro-tactile display with a Sensory PCI card and a group of power sources and amplifiers and designed a multi-armed bandit-based calibration method to find an optimal signal parameter for pleasant stimulation.
      <br><br>
      <a href="https://github.com/xinleiyuUSC/MAB_UCB">[Github(partially available)]</a>
    </td>
  </tr>
  <!--<tr>
    <td style="width:40%">
      <img src="https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/TTT.png" alt="drawing2" height="200" width="400"/>
    </td>
    <td style="width:60%">
      <strong>Tummy Time Toy:</strong> A computer vision-based infant motor learning assistant toy (under US Patent review). This interactive toy rewards infants with lights and music when they lift their heads past a certain threshold, encouraging the development of prone motor skills. The primary goal is to study whether babies can learn to control their bodies during tummy time with the toy's assistance, aiding in muscle control and increasing their tolerance for tummy time.
      <br><br>
      <a href="https://youtu.be/6PznLd5wy5c">[video]</a> [Github(available soon)]
    </td>
  </tr> -->
  <tr>
    <td style="width:40%">
      <img src="https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/ASL.jpg" alt="drawing3" height="200" width="400"/>
    </td>
    <td style="width:60%">
      <strong>American Sign Language:</strong> we present an alphabet translator for American Sign Language (ASL), deploying Convolutional Neural Networks (CNN) and Residual Neural Networks (ResNet) to classify RGB images of ASL alphabet hand gestures. We meticulously tuned hyperparameters to ensure high training accuracy and solid test performance.
      <br><br>
      <a href="https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/ASL_Paper.pdf">[paper]</a> <a href="https://github.com/xinleiyuUSC/ASL_Project/tree/main">[Github]</a>
    </td>
  </tr>
</table>
